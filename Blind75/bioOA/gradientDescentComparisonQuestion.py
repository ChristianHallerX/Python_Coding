"""
Gradient descent comparison

Which of the following statements is true about stochastic, batch, and mini-batch gradient descent?

- Compared to stochastic gradient descent, mini-batch gradient descent tends to result in noisier gradients when
    updating model weights

- Batch gradient descent is much faster than stochastic gradient descent for training on very large datasets

- Stochastic gradient descent will update the model's weights only after going through each training example in the
    dataset

- The batch size in mini-batch gradient descent is often treated as an additional tunable hyperparameter
"""

# Answer
" The batch size in mini-batch gradient descent is often treated as an additional tunable hyperparameter"
